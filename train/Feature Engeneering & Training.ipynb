{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "fullset = pd.read_csv('hatespeech-bruehl-momenzada.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "offensive    19712\n",
       "hate         17033\n",
       "nothing       4163\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullset['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "Unfortunately, this dataset has no emojis.\n",
    "\n",
    "We do not remove stopwords (such as i, and, myself, etc. which are included in the ntlk stopwords list), since this can distort the context of a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_series(corpus:pd.Series) -> pd.Series:\n",
    "    #get rid of further symbols \n",
    "    corpus = corpus.replace({'[»„‘’“”…]': ' '}, regex=True)\n",
    "    \n",
    "    #get rid of digits\n",
    "    corpus = corpus.replace({'\\w*\\d\\w*': 'Nummer'}, regex=True)\n",
    "\n",
    "    # get rid of urls\n",
    "    corpus = corpus.replace({r\"https?://\\S+|www\\.\\S+\": ' '}, regex=True)\n",
    "\n",
    "    #get not identified unicode\n",
    "    corpus = corpus.replace('[\\u0080-\\uffff]w{1-3}', \" \", regex=True)\n",
    "    \n",
    "    #delete /t and /n\n",
    "    corpus = corpus.replace('/t', \" \", regex=True)\n",
    "    corpus = corpus.replace('/n', \" \", regex=True)\n",
    "\n",
    "    # replaces all stringw which are unicodes (\\u2009 \\a0x) and also removes bashes\n",
    "    corpus = corpus.replace({r\"[^\\x00-\\x7F\\w{1,3}]+\": ' '}, regex=True)\n",
    "    \n",
    "    # remove @usernames\n",
    "    corpus = corpus.replace({r\"(#[\\d\\w\\.]+)\": ' '}, regex=True)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "fullset['text'] = clean_series(fullset['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Labels as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define labels\n",
    "lb_make = LabelEncoder()\n",
    "fullset[\"label_id\"] = lb_make.fit_transform(fullset[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59667 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of most frequent words to be used.\n",
    "MAX_NB_WORDS = 59000\n",
    "# Max number of words in each row\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "#feature vector length\n",
    "HIDDEN_DIM = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(fullset['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('tokenizer.pkl','wb') as f:\n",
    "    pkl.dump(tokenizer, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (40908, 150)\n",
      "Shape of label tensor: (40908, 3)\n"
     ]
    }
   ],
   "source": [
    "# define X and Y\n",
    "X = tokenizer.texts_to_sequences(fullset['text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "Y = pd.get_dummies(fullset['class']).values\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "# a really messy way to save labels\n",
    "onehot_labels = pd.get_dummies(fullset['class'])\n",
    "labels = onehot_labels.drop_duplicates(subset=onehot_labels.columns).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('offensive_hate.pkl','wb') as f:\n",
    "    pkl.dump(X, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and saving path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import os\n",
    "\n",
    "checkpoint_path = \"model_pretrain/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                  save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding,SpatialDropout1D, LSTM,Conv1D,MaxPooling1D\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def hate_offensive_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NB_WORDS, HIDDEN_DIM, input_length=X.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "216/216 [==============================] - 187s 856ms/step - loss: 0.4653 - accuracy: 0.8167 - val_loss: 0.3029 - val_accuracy: 0.8886\n",
      "\n",
      "Epoch 00001: saving model to model_pretrain\\cp.ckpt\n",
      "Epoch 2/5\n",
      "216/216 [==============================] - 208s 961ms/step - loss: 0.2019 - accuracy: 0.9300 - val_loss: 0.2547 - val_accuracy: 0.9058\n",
      "\n",
      "Epoch 00002: saving model to model_pretrain\\cp.ckpt\n",
      "Epoch 3/5\n",
      "216/216 [==============================] - 203s 941ms/step - loss: 0.1210 - accuracy: 0.9611 - val_loss: 0.2927 - val_accuracy: 0.8993\n",
      "\n",
      "Epoch 00003: saving model to model_pretrain\\cp.ckpt\n",
      "Epoch 4/5\n",
      "216/216 [==============================] - 230s 1s/step - loss: 0.0753 - accuracy: 0.9770 - val_loss: 0.3283 - val_accuracy: 0.8967\n",
      "\n",
      "Epoch 00004: saving model to model_pretrain\\cp.ckpt\n",
      "Epoch 5/5\n",
      "216/216 [==============================] - 308s 1s/step - loss: 0.0517 - accuracy: 0.9841 - val_loss: 0.3671 - val_accuracy: 0.8915\n",
      "\n",
      "Epoch 00005: saving model to model_pretrain\\cp.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x237cd9665e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = hate_offensive_model()\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "model.fit(X_train, Y_train,epochs=epochs, batch_size=batch_size,validation_split=0.1, callbacks=[cp_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
